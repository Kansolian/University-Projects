{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54699ecd-7d62-4a55-b3d3-6467618d3b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "from numpy.fft import fft, ifft         \n",
    "import pandas as pd                 \n",
    "from os import getcwd, listdir, chdir \n",
    "from os.path import join\n",
    "import math, random\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e483139-a32c-409b-b377-5a0a0c99b4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Id  Label\n",
      "0     xc132608    154\n",
      "1     xc132609    154\n",
      "2     xc132611    154\n",
      "3      xc26308    154\n",
      "4     xc168551    154\n",
      "...        ...    ...\n",
      "4126   xc97603    173\n",
      "4127   xc97604    173\n",
      "4128   xc40203    112\n",
      "4129   xc29563    112\n",
      "4130  xc135855    112\n",
      "\n",
      "[4131 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#path needs to be changed on local pc\n",
    "pathClassiBird=\"C:/Users/friez/OneDrive - Tilburg University/CSAI/Year_3_Sem_1/Software_Engineering/Project/Dataset/BirdSoundsClassif\"\n",
    "dfDump = pd.read_csv(join(pathClassiBird,\"xcmeta.csv\"), skipinitialspace=True, usecols=[\"Concat\"])\n",
    "strings = []\n",
    "\n",
    "for row in range(len(dfDump)):\n",
    "    strings.append(dfDump.loc[row,\"Concat\"])\n",
    "    \n",
    "for s in range(len(strings)):\n",
    "    index = s\n",
    "    s = strings[index]\n",
    "    totalSplit = s.split(\"\\t\")\n",
    "    strings[index] = totalSplit\n",
    "\n",
    "dataLabelInf = pd.DataFrame(strings,columns =['Id', 'Scientific Name1',\"Scientific Name2\",\"Common Name\",\"Finder\",\"Country\",\"Longitude\",\"Latitude\",\"Type\",\"License\"])\n",
    "\n",
    "dataLabelInf[\"Scientific Name1\"] = dataLabelInf[\"Scientific Name1\"] +\" \"+ dataLabelInf[\"Scientific Name2\"]\n",
    "dataLabelInf.rename(columns={'Scientific Name1': 'Scientific Name'}, inplace=True)\n",
    "dataLabelInf=dataLabelInf.drop(columns=\"Scientific Name2\")\n",
    "dataLabelInf[\"Id\"]=\"xc\"+dataLabelInf[\"Id\"]\n",
    "\n",
    "unique =set()\n",
    "for i in dataLabelInf[\"Common Name\"]:\n",
    "    unique.add(i)\n",
    "\n",
    "lbls=dict()\n",
    "unique= list(unique)\n",
    "for i in range(len(unique)):\n",
    "    lbls[unique[i]]=i\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(dataLabelInf[\"Common Name\"])):\n",
    "    dataLabelInf.loc[i,\"Label\"] = lbls[dataLabelInf.loc[i,\"Common Name\"]]\n",
    "\n",
    "dataLabelInf['Label'] = dataLabelInf['Label'].astype(np.int64)\n",
    "\n",
    "cleanFolder = join(pathClassiBird,\"flac\")\n",
    "cleanFolder = listdir(cleanFolder)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in dataLabelInf[\"Id\"].values:\n",
    "    if i+\".flac\" in cleanFolder:\n",
    "        continue\n",
    "    else:\n",
    "        counter +=1\n",
    "        dataLabelInf = dataLabelInf.drop(dataLabelInf[dataLabelInf[\"Id\"] == i].index)\n",
    "\n",
    "dataLabelInf = dataLabelInf.reset_index(drop=True)\n",
    "dataLabelInf = dataLabelInf[[\"Id\",\"Label\"]]\n",
    "print(dataLabelInf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ef8904-c093-4598-a978-41ed2bc1daa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.]]), 44100)\n"
     ]
    }
   ],
   "source": [
    "path = join(pathClassiBird,\"flac\",\"xc132608.flac\")\n",
    "print(AudioUtil.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6c873a-5bb9-480e-b5cd-981b081f5b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    # checking assumptions before preprocessing the data\n",
    "\n",
    "    # open audio file\n",
    "    def open(path):\n",
    "        sig, sr = torchaudio.load(path)\n",
    "        return (sig, sr)\n",
    "\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sig.shape[0] == new_channel):\n",
    "        # Nothing to do\n",
    "            return aud\n",
    "\n",
    "        if (new_channel == 1):\n",
    "          # Convert from stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "          # Convert from mono to stereo by duplicating the first channel\n",
    "            resig = torch.cat([sig, sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "      # Nothing to do\n",
    "          return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "    # Resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "      # Resample the second channel and merge both channels\n",
    "          retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "          resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "            sig = sig[:,:max_len]\n",
    "\n",
    "        elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "          pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "          pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "          pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "          pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "          sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "        return (sig, sr)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig,sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8bc5bc00-6dc6-4bb6-b7fa-e67e07a328e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "    self.duration = 20000\n",
    "    self.sr = 44100\n",
    "    self.channel = 2\n",
    "    self.shift_pct = 0.4\n",
    "            \n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Absolute file path of the audio file - concatenate the audio directory with\n",
    "    # the relative path\n",
    "    audio_file = join(self.data_path,\"flac\",self.df.loc[idx, 'Id'])+\".flac\"\n",
    "    # Get the Class ID\n",
    "    class_id = self.df.loc[idx, 'Label']\n",
    "\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "    # majority. So make all sounds have the same number of channels and same \n",
    "    # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "    # result in arrays of different lengths, even though the sound duration is\n",
    "    # the same.\n",
    "    reaud = AudioUtil.resample(aud, self.sr)\n",
    "    rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "    # these are not nessecary because they are already standardized an have the same dimension\n",
    "    # see checking assumptions above\n",
    "\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "    return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ba243489-7aff-46c9-869b-a20e824be2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting soundData into test, train and validition sets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "myds = SoundDS(dataLabelInf, pathClassiBird)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "\n",
    "\n",
    "# Create training and validation data loaders\n",
    "batchSize = 256\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size= batchSize,shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae51ab47-6bb9-46c6-89a2-c52369e97880",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Fourth Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=len(unique))\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ca1b4254-abd2-40d4-952d-b19dff32dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 5.61, Accuracy: 0.01\n",
      "Epoch: 1, Loss: 5.53, Accuracy: 0.05\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print('Finished Training')\n",
    "  \n",
    "num_epochs=2   # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7037db-7ba5-4a93-b5f5-eae0f4a6382a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "04c088fe-351b-4dd5-8f4e-ff8b957fdcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.05, Total items: 826\n"
     ]
    }
   ],
   "source": [
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "    \n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434603c-2dcc-4a86-9fc3-d4bd4b3bba1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
