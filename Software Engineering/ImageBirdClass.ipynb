{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337adfc1-24a6-4a95-86da-3583dbef5862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import read, write\n",
    "from IPython.display import Audio\n",
    "import torch\n",
    "from numpy.fft import fft, ifft         \n",
    "import pandas as pd                 \n",
    "from os import getcwd, listdir, chdir \n",
    "from os.path import join\n",
    "import math, random\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "014d8bc5-f2d3-4ccf-95c6-e4c00be2f834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Set                    Label Numeric Label Path\n",
      "0       test          ABBOTTS BABBLER             0    1\n",
      "1       test          ABBOTTS BABBLER             0    2\n",
      "2       test          ABBOTTS BABBLER             0    3\n",
      "3       test          ABBOTTS BABBLER             0    4\n",
      "4       test          ABBOTTS BABBLER             0    5\n",
      "...      ...                      ...           ...  ...\n",
      "75121  valid  YELLOW HEADED BLACKBIRD           449    1\n",
      "75122  valid  YELLOW HEADED BLACKBIRD           449    2\n",
      "75123  valid  YELLOW HEADED BLACKBIRD           449    3\n",
      "75124  valid  YELLOW HEADED BLACKBIRD           449    4\n",
      "75125  valid  YELLOW HEADED BLACKBIRD           449    5\n",
      "\n",
      "[75126 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "pathClassiBird=\"C:/Users/friez/OneDrive - Tilburg University/CSAI/Year_3_Sem_1/Software_Engineering/Project/Dataset/BirdImage\"\n",
    "\n",
    "\n",
    "unique = []\n",
    "difSet = ['test','train', 'valid']\n",
    "labelAll =[]\n",
    "imageData = pd.DataFrame()\n",
    "\n",
    "lbls=dict()\n",
    "unique= listdir(join(pathClassiBird,difSet[1]))\n",
    "for i in range(len(unique)):\n",
    "    lbls[unique[i]]=i\n",
    "\n",
    "for a in range(len(difSet)):\n",
    "    difSetPath = join(pathClassiBird,difSet[a])\n",
    "    l = listdir(difSetPath)\n",
    "    for i in range(len(l)):\n",
    "        label = l[i]\n",
    "        imagePath = join(difSetPath,label)\n",
    "        imageList = listdir(imagePath)\n",
    "        for j in range(len(imageList)):\n",
    "            frame = [difSet[a]]\n",
    "            frame.append(label)\n",
    "            frame.append(lbls[label])\n",
    "            frame.append(imageList[j].split(\".\")[0])\n",
    "            imageData=pd.concat([imageData,pd.Series(frame).to_frame().T],ignore_index=True)\n",
    "\n",
    "imageData = imageData.rename(columns={0: \"Set\", 1: \"Label\", 2:\"Numeric Label\", 3: \"Path\"} )           \n",
    "print(imageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4bf2500-f811-48c3-9b5a-c60cb8772e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "class ImageUtil():\n",
    "    # checking assumptions before preprocessing the data\n",
    "\n",
    "    # open audio file\n",
    "    def open(path):\n",
    "        img = Image.open(path)\n",
    "        return (img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78bae8a-feae-498c-b81e-e96840ea6aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = join(pathClassiBird,\"test\", \"ABBOTTS BABBLER\",  \"1.jpg\")\n",
    "im = ImageUtil.open(path)\n",
    "im = im.resize((300,300))\n",
    "im = torch.tensor(np.array(im))\n",
    "print(im.permute(2,0,1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e0e58e9-d96c-4c03-b515-6e5f5580dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class ImageDS(Dataset):\n",
    "  def __init__(self, df, data_path):\n",
    "    self.df = df\n",
    "    self.data_path = str(data_path)\n",
    "            \n",
    "  # ----------------------------\n",
    "  # Number of items in dataset\n",
    "  # ----------------------------\n",
    "  def __len__(self):\n",
    "    return len(self.df)    \n",
    "    \n",
    "  # ----------------------------\n",
    "  # Get i'th item in dataset\n",
    "  # ----------------------------\n",
    "  def __getitem__(self, idx):\n",
    "    # Get image in PIL format\n",
    "    image_file = join(self.data_path, self.df.loc[idx,\"Set\"] ,self.df.loc[idx, \"Label\"],self.df.loc[idx, 'Path'])+\".jpg\"\n",
    "    # Get the Class ID\n",
    "    class_id = self.df.loc[idx, 'Numeric Label']\n",
    "\n",
    "    img = ImageUtil.open(image_file)\n",
    "    img = img.resize((300,300))\n",
    "    img = torch.tensor(np.array(img)).permute(2,0,1)\n",
    "    return img.float(), class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02beaa0c-b39f-44b4-8b8e-05de88e2e148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d53666-014e-4c58-8893-8e8303e5ea60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDS(imageData[imageData[\"Set\"]==\"train\"].reset_index(), pathClassiBird)\n",
    "test_ds  = ImageDS(imageData[imageData[\"Set\"]==\"test\"].reset_index(), pathClassiBird)\n",
    "val_ds   = ImageDS(imageData[imageData[\"Set\"]==\"valid\"].reset_index(), pathClassiBird)\n",
    "\n",
    "\n",
    "# Create training and validation data loaders\n",
    "batchSize = 32\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size= batchSize,shuffle=True,pin_memory = True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=batchSize, shuffle=False,pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d772ea7-61e0-4fc7-9a13-3fbe45287210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.float()/255)                 # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)            # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images.float()/255)                 # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d17f691-66f9-4c48-bda1-ec78a61fb13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "\n",
    "class NaturalSceneClassification(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(350464,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,450)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)\n",
    "    \n",
    "#simple_model = nn.Sequential(\n",
    " #   nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
    " #   nn.MaxPool2d(2, 2)\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f12372-db1b-44bf-85cf-bc37997f1d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afe15fbf-b4fc-4070-90aa-fbcb97fe497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503b7fc-aa0c-4080-a568-ebdca2c68462",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaturalSceneClassification()\n",
    "num_epochs = 2\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)\n",
    "\n",
    "\n",
    "def plot_accuracies(history):\n",
    "    \"\"\" Plot the history of accuracies\"\"\"\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    plt.plot(accuracies, '-x')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Accuracy vs. No. of epochs');\n",
    "    \n",
    "\n",
    "plot_accuracies(history)\n",
    "\n",
    "def plot_losses(history):\n",
    "    \"\"\" Plot the losses in each epoch\"\"\"\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');\n",
    "\n",
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa2b6f-d75c-4930-8711-0fa6c4c5c3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3164f8-137a-4051-825d-6218abf3f4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
