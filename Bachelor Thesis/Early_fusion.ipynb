{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb8a33-e474-491b-8176-ccf9d563b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from random import shuffle, sample\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import SubsetRandomSampler, ConcatDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "from torch.utils.data import Dataset \n",
    "\n",
    "from platform import python_version\n",
    " \n",
    " \n",
    "print(\"Current Python Version-\", python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d70f98",
   "metadata": {},
   "source": [
    "1.23.5 np\n",
    "2.0.0 torch\n",
    "print(Image.__version__) 9.4.0 pillow\n",
    "print(plt.__version__) 3.7.1\n",
    "print(transform.__version__) 0.15.0 torchvision\n",
    "print(sklearn.__version__) 1.2.2 scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea24b7-d9e5-4815-bdcc-47ecfe1465ab",
   "metadata": {},
   "source": [
    "# Action recognition based on image inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133cce5-bb01-4132-a117-e4c2530306b0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686e0cc-5ba6-4439-9f45-c25d74a2cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"action_recognition_dataset\"\n",
    "tools = [\"spatula\",\"ruler\",\"hook\",\"sshot\"]\n",
    "actions = [\"left_to_right\",\"pull\",\"push\",\"right_to_left\"]\n",
    "objects = ['woodenCube', 'tomatoCan', 'boxMilk', 'containerNuts', 'cornCob', 'yellowFruitToy', 'bottleNailPolisher', 'boxRealSense', 'clampOrange', 'greenRectangleToy', 'ketchupToy', 'peartoy', 'yogurtYellowbottle', 'cowToy', 'tennisBallYellowGreen', 'blackCoinBag', 'lemonSodaCan', 'peperoneGreenToy', 'boxEgg', 'pumpkinToy']\n",
    "\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465dd197-e467-4ab2-9ee2-f36ec3d9d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openImage(path):\n",
    "    image = None\n",
    "    transform = transforms.Compose([transforms.Resize((64,64)),\n",
    "                                     transforms.ToTensor()\n",
    "                                    ])\n",
    "    with Image.open(path) as im:\n",
    "        image=im.convert(\"RGB\")\n",
    "        image= transform(image)\n",
    "        mean= image.mean([1,2])\n",
    "        std = image.std([1,2])\n",
    "        if std[0]==0:\n",
    "            std[0]=1\n",
    "        image = transforms.Normalize(mean,std)(image)\n",
    "    return image\n",
    "\n",
    "def setDataframe(path, tools, actions):\n",
    "    dataset = pd.DataFrame()\n",
    "    directories = os.listdir(path)\n",
    "    \n",
    "    \n",
    "    for iFolder in range(len(directories)):\n",
    "        objs = directories[iFolder]\n",
    "        obj = objs.split(\"_\")[1]\n",
    "        objects.append(obj)\n",
    "        for act in actions:\n",
    "            for tool in tools:\n",
    "                for i in range(10):\n",
    "                    path = os.path.join(dataPath,objs,tool,act)\n",
    "                    initColor = openImage(os.path.join(path,\"color\",f\"init_color_{i}.png\"))\n",
    "                    initDepth = openImage(os.path.join(path,\"depthcolormap\",f\"init_depthcolormap_{i}.png\"))\n",
    "                    effectColor = openImage(os.path.join(path,\"color\",f\"effect_color_{i}.png\"))\n",
    "                    effectDepth = openImage(os.path.join(path,\"depthcolormap\",f\"effect_depthcolormap_{i}.png\"))\n",
    "                    \n",
    "                    frames = [obj,tool,act,initColor,effectColor,initDepth,effectDepth]\n",
    "                    dataset=pd.concat([dataset,pd.Series(frames).to_frame().T],ignore_index=True)\n",
    "    \n",
    "    dataset = dataset.rename(columns ={0: \"Object\" ,1: \"Tool\",2:\"Action\", 3:\"Color Initial\",4:\"Color Effect\",5:\"Depthcolor Initial\",6:\"Depthcolor Effect\"})\n",
    "    return dataset\n",
    "        \n",
    "def label_encode(entries, labels):\n",
    "    dictL = dict()\n",
    "    res = list()\n",
    "    for i, label in enumerate(labels):\n",
    "        dictL[label] = i\n",
    "    \n",
    "    for entrie in entries:\n",
    "        res.append(dictL[entrie])\n",
    "    \n",
    "    return pd.Series(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0b6390-e19a-49c3-a90a-51a796c56be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = setDataframe(dataPath,tools,actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d66ad-be28-47c7-b49e-6d04e6e27610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cefab2-68c4-4a30-9338-3e62bcce8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset[\"ActionL\"] = label_encode(dataset[\"Action\"],actions)\n",
    "dataset[\"ToolL\"]   = label_encode(dataset[\"Tool\"],tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2a755-f5b3-4535-bb9b-3df1afac9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dataset[\"Color Initial\"][0]\n",
    "\n",
    "print(\"Output:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2705eca3-f4f7-4759-8272-0260a6518949",
   "metadata": {},
   "source": [
    "# Early Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a200bba-53c2-4727-be84-e38e45a12414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(numOutputC, n_layerC, numOutputH, n_layerH):\n",
    "    model=nn.Sequential()\n",
    "    model.add_module(\"Conv0\",nn.Conv2d(3, numOutputC[0], kernel_size = 3, padding = 1))\n",
    "    model.add_module(\"Relu0\",nn.ReLU())\n",
    "    model.add_module(\"Norm0\",nn.BatchNorm2d(numOutputC[0]))\n",
    "    model.add_module(\"MaxPool0\",nn.MaxPool2d(2,2))\n",
    "    \n",
    "    i=0\n",
    "    while i < n_layerC-1:\n",
    "        model.add_module(f\"Conv{i+1}\",nn.Conv2d(numOutputC[i], numOutputC[i+1], kernel_size = 3, padding = 1))\n",
    "        model.add_module(f\"Relu{i+1}\",nn.ReLU())\n",
    "        model.add_module(f\"Norm{i+1}\",nn.BatchNorm2d(numOutputC[i+1]))\n",
    "        model.add_module(f\"MaxPool{i+1}\",nn.MaxPool2d(2,2))\n",
    "        i+=1\n",
    "        \n",
    "    model.add_module(\"Flatten\", nn.Flatten())\n",
    "\n",
    "#(2**(18-n_layerC)\n",
    "\n",
    "    model.add_module(\"Drop\",nn.Dropout(0.4))\n",
    "    model.add_module(\"Linear\",nn.Linear(2**(18-n_layerC),numOutputH[0]))\n",
    "    model.add_module(\"ReluH0\",nn.ReLU())\n",
    "    model.add_module(\"NormH\",nn.BatchNorm1d(numOutputH[0]))\n",
    "    i=0\n",
    "    while i< n_layerH-1:\n",
    "        \n",
    "        model.add_module(f\"Drop{i+1}\",nn.Dropout(0.4))\n",
    "        model.add_module(f\"Linear{i+1}\",nn.Linear(numOutputH[i], numOutputH[i+1]))\n",
    "        model.add_module(f\"ReluH{i+1}\",nn.ReLU())\n",
    "        model.add_module(f\"NormH{i+1}\",nn.BatchNorm1d(numOutputH[i+1]))\n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class ImageEarlyFClassifier(nn.Module):\n",
    "    def __init__(self, numOutputC, n_layerC, numOutputH, n_layerH):\n",
    "        super().__init__()\n",
    "        self.network = build(numOutputC, n_layerC, numOutputH, n_layerH)\n",
    "        self.fc1= nn.Linear(numOutputH[-1],4)\n",
    "        self.fc2= nn.Linear(numOutputH[-1],4)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, xb):\n",
    "        xb = self.network(xb)\n",
    "        label2 = self.fc1(xb)\n",
    "        label1 = self.fc2(xb)\n",
    "        return {'Tool': label1, 'Action': label2}\n",
    "\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch,crit):\n",
    "        images, l1,l2, = batch\n",
    "        images, l1,l2= images.to(device), l1.to(device), l2.to(device)\n",
    "        \n",
    "        out = self(images)     \n",
    "        \n",
    "        loss1 = crit(out[\"Action\"], l1)\n",
    "        loss2 = crit(out[\"Tool\"], l2)\n",
    "        return loss1, loss2\n",
    "    \n",
    "    def validation_step(self, batch,crit):\n",
    "        images, l1,l2 = batch\n",
    "        images, l1,l2= images.to(device), l1.to(device), l2.to(device)\n",
    "        \n",
    "        out = self(images)     \n",
    "        \n",
    "        loss1 = crit(out[\"Action\"], l1)\n",
    "        loss2 = crit(out[\"Tool\"], l2) \n",
    "        loss = loss1+loss2\n",
    "        \n",
    "\n",
    "        acc1 = accuracy(out[\"Action\"], l1)\n",
    "        acc2 = accuracy(out[\"Tool\"], l2) \n",
    "        acc = (acc1+acc2)/2\n",
    "        return {'val_loss': loss.detach(), \"val_acc\": acc, 'val_acc_Action': acc1, 'val_acc_Tool': acc2}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      \n",
    "        batch_accs1 = [x['val_acc_Tool'] for x in outputs]\n",
    "        epoch_acc1 = torch.stack(batch_accs1).mean()\n",
    "        batch_accs2 = [x['val_acc_Action'] for x in outputs]\n",
    "        epoch_acc2 = torch.stack(batch_accs2).mean()\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item(), 'val_acc_Tool': epoch_acc1.item(), 'val_acc_Action': epoch_acc2.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}, val_acc_Tool: {:.4f}, val_acc_Action: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'],result['val_acc'],result['val_acc_Tool'],result['val_acc_Action']))\n",
    "            \n",
    "            \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "\n",
    "def evaluate(model, val_loader,crit):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch,crit) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "  \n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD, crit =nn.CrossEntropyLoss()):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss1, loss2 = model.training_step(batch,crit)\n",
    "            loss = loss1+loss2\n",
    "            \n",
    "            train_losses.append(loss)\n",
    "                     \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        \n",
    "        result = evaluate(model, val_loader,crit)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "        if result[\"train_loss\"] - result[\"val_loss\"] < -0.03:\n",
    "            break\n",
    "        \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc49957-c780-46de-8076-928bc9209ac6",
   "metadata": {},
   "source": [
    "## Dataset adaption for early fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eedede8-955e-47a2-90e0-22b15e59fb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinTensor(*args):\n",
    "    l = list()\n",
    "    for i in range(len(args[0])):\n",
    "        newT = torch.cat((args[0][i],args[1][i],args[2][i],args[3][i]),1)\n",
    "        l.append(newT)\n",
    "    s = pd.Series(l)\n",
    "    return s\n",
    "\n",
    "earlyD = pd.DataFrame()\n",
    "earlyD[\"fused\"] = joinTensor(dataset[\"Color Initial\"],dataset[\"Color Effect\"],dataset[\"Depthcolor Initial\"],dataset[\"Depthcolor Effect\"])\n",
    "earlyD[[\"ActionL\",\"ToolL\"]]= dataset[[\"ActionL\",\"ToolL\"]]\n",
    "print(len(earlyD[\"fused\"][0][0]))\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc7aed3-c4fc-4474-a397-dd60dd0fe609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    " \n",
    "    def __init__(self,df):\n",
    "        df=df\n",
    " \n",
    "        x=df.iloc[:,0].values\n",
    "        y=df.iloc[:,1].values\n",
    "        y2=df.iloc[:,2].values\n",
    "\n",
    "        \n",
    "        \n",
    " \n",
    "        self.x_train=x\n",
    "        self.y_train=y\n",
    "        self.y2_train=y2\n",
    "\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "   \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx],self.y2_train[idx]\n",
    "    \n",
    "    def getX(self):\n",
    "        return self.x_train\n",
    "    \n",
    "    def getYA(self):\n",
    "        return self.y_train\n",
    "    \n",
    "    def getYT(self):\n",
    "        return self.y2_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9104e5-fa28-4878-8e9e-b5f00f643422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_data,val_data = train_test_split(earlyD, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(val_data,test_size=0.5, random_state=42)\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "train_data = MyDataset(train_data)\n",
    "val_data = MyDataset(val_data)\n",
    "test_data = MyDataset(test_data)\n",
    "\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size)\n",
    "val_dl = DataLoader(val_data, batch_size*2)\n",
    "test_dl = DataLoader(test_data,480)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f86413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies_losses(history,title):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "\n",
    "    accuracies = [x['val_acc'] for x in history]\n",
    "    ax1.plot(accuracies, '-x')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.set_ylabel('accuracy')\n",
    "    ax1.set_title('Accuracy vs. No. of epochs')\n",
    "    \n",
    "\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    ax2.plot(train_losses, '-bx')\n",
    "    ax2.plot(val_losses, '-rx')\n",
    "    ax2.set_xlabel('epoch')\n",
    "    ax2.set_ylabel('loss')\n",
    "    ax2.legend(['Training', 'Validation'])\n",
    "    ax2.set_title('Loss vs. No. of epochs')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6301d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3697924",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da10239",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.0001\n",
    "numOutputH = [64,32,16]\n",
    "numOutputC = [32,64,128]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "k=10\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "foldperf={}\n",
    "models = []\n",
    "history = {'train_loss': [], 'test_loss': [],'test_acc_Tool':[],'test_acc_Action':[],'test_acc':[]}\n",
    "dataset = ConcatDataset([train_data, val_data])\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "\n",
    "    inH = numOutputH[:]\n",
    "    inH.reverse()\n",
    "    \n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(val_idx)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
    "    \n",
    "    model = ImageEarlyFClassifier(numOutputC, len(numOutputC), inH, len(inH))\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam\n",
    "    h = fit(epoch, lr, model, train_loader, test_loader, optimizer)\n",
    "    \n",
    "    dif = [(x['train_loss']-x['val_loss'],x['val_acc'],x['train_loss'],x['val_loss'],x['val_acc_Tool'],x['val_acc_Action']) for x in h]\n",
    "    \n",
    "    models.append(model)\n",
    "    history['train_loss'].append(dif[-1][2])\n",
    "    history['test_loss'].append(dif[-1][3])\n",
    "    history['test_acc_Tool'].append(dif[-1][4])\n",
    "    history['test_acc_Action'].append(dif[-1][5])\n",
    "    history['test_acc'].append(dif[-1][1])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571c534",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss = np.mean(history['train_loss'])\n",
    "avg_test_loss = np.mean(history['test_loss'])\n",
    "avg_test_acc = np.mean(history['test_acc'])\n",
    "avg_test_acc_Tool = np.mean(history['test_acc_Tool'])\n",
    "avg_test_acc_Action = np.mean(history['test_acc_Action'])\n",
    "\n",
    "print('Performance of {} fold cross validation'.format(k))\n",
    "print(\"Average Training Loss: {:.4f} \\t Average Test Loss: {:.4f}  \\t Average Test Acc: {:.3f} \\t Average Test Acc Tool: {:.3f} \\t Average Test Acc Action: {:.3f}\".format(avg_train_loss,avg_test_loss,avg_test_acc,avg_test_acc_Tool,avg_test_acc_Action))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab7299",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestM = max(history[\"test_acc_Tool\"])\n",
    "print(bestM)\n",
    "print(history[\"test_acc_Tool\"])\n",
    "\n",
    "print(history[\"test_acc_Tool\"].index(bestM))\n",
    "bestM=history[\"test_acc_Tool\"].index(bestM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015e143-eb02-4ab6-b4c4-fd182f2abd74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab75b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3c8ddb-2629-47c0-84bf-b9a167c74c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[bestM]\n",
    "\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    model.eval() \n",
    "    \n",
    "    for images, l1, l2 in loader:\n",
    "        images, l1,l2 = images.to(device), l1.to(device), l2.to(device)\n",
    "        \n",
    "        preds = model(images)\n",
    "        predsA = preds[\"Action\"].to(\"cpu\")\n",
    "        predsT = preds[\"Tool\"].to(\"cpu\")\n",
    "        all_predsA = torch.cat((all_preds, predsA), dim=0)\n",
    "        all_predsT = torch.cat((all_preds, predsT), dim=0)\n",
    "    return all_predsA, all_predsT\n",
    "\n",
    "\n",
    "with torch.no_grad(): \n",
    "    predsA, predsT = get_all_preds(model, test_dl)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cmA = confusion_matrix(test_data.getYA(), predsA.argmax(dim=1))\n",
    "cmT = confusion_matrix(test_data.getYT(), predsT.argmax(dim=1))\n",
    "\n",
    "\n",
    "report = classification_report(test_data.getYA(),predsA.argmax(dim=1), target_names=['push', 'pull', 'left to right', \"right to left\"], zero_division= 0)\n",
    "print('Classification Report A: ', report)\n",
    "\n",
    "report = classification_report(test_data.getYT(),predsT.argmax(dim=1), target_names=['sshot', 'spatula', 'hook', \"ruler\"], zero_division= 0)\n",
    "print('Classification Report T: ', report)\n",
    "    \n",
    "\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cmA, display_labels=['push', 'pull', 'left to right', \"right to left\"])\n",
    "display.plot()\n",
    "plt.show()\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cmT, display_labels=['sshot', 'spatula', 'hook', \"ruler\"])\n",
    "display.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
