{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfead40",
   "metadata": {},
   "source": [
    "# Spelling correction\n",
    "\n",
    "You receive the following files, available via SurfDrive:\n",
    "- sentences_with_typos.txt: a file with two tab-separated columns, the first containing a numerical ID and the second containing a sentence in English\n",
    "- SUBTLEXus.txt: a file with several tab-separated columns, containing data from the SUBTLEXus lexicon, a list of words derived from a part of the SUBTLEXus corpus containing movie subtitles with their frequency counts (and other pieces of information)\n",
    "- LM_trainingCorpus.json: a file containing a pre-processed corpus in the form of a list of lists of strings, to be used to train a word-level statistical language model\n",
    "\n",
    "You should carry out the following tasks:\n",
    "\n",
    "- task1: comment the code to estimate the LM by adding doc-strings. 4 points available, you lose 1 point for every missing docstring. There are 6 docstrings to write, so writing 2 give you the same points as writing none: the rationale behind the grading approach is that writing only 2 is not a sufficient demonstration you understand the code.\n",
    "\n",
    "\n",
    "- task2a: After reading in the files as pandas dataframes (mind how you import default strings, some of them may turn into NAs!), find which token contains a typo in the given sentences, where having a typo means that the word does not feature in SUBTLEXus - remember to tokenise the input sentences! There is one and only one word containing a typo in each sentence, as defined in this way: the typo can result from the insertion/deletion/substitution of one or two characters. You should submit a .json file containing a simple dictionary mapping the sentence ID (as an integer) to the mistyped word (as a string). 5 points available in total: for every incorrect word retrieved, you lose 0.5 point. If you retrieve 10 wrong mistyped words, you get no points even if the total number of mistyped words is higher than 10. If you submit a wrongly formatted file, you will get no points.\n",
    "\n",
    "\n",
    "- task2b: read the target sentences manually. Some of them contain another mistyped word than the one you found in task2a, but were not detected because the typo resulted in a word which appears in SUBTLEXus. Discuss how you could automatically spot those mistyped words too using CL methods and resources: what information would you need? How would you use it? Reply in no more than 150 words. 3 points available, awarded based on how sensible the answer is.\n",
    "\n",
    "\n",
    "- task3: find the words from SUBTLEXus with the smallest edit distance from each mistyped target (do not lowercase anything). You should return the 3 words at the smallest edit distance, sorted by edit distance. However, if there are more words at the same edit distance than the third closest, you should include all the words at the same edit distance. Therefore, supposing that the string 'abcdef' has two neighbors at edit distance 1, and four neighbors at edit distance 2, the third closest neighbor would be at edit distance 2, but there would be other three words at the same distance and you should thus return six neighbors for the target string. 5 points available: you loose 0.5 points for every wrong list of nearest neighbors you retrieve for a mistyped word (wrong means any of wrong words, wrong order, wrong edit distances, wrong data type). Submit a .json file containing a dictionary mapping sentence IDs (as integers) to lists of tuples, where each tuple contains first the word (as a string) and then the edit distance (as an integer), with tuples sorted by edit distance in ascending order (smallest edit distance first). The dictionary should have the following form (if you submit a wrongly formatted file, you will get no points):\n",
    "    {id1: [(w1, 2), (w2, 2), (w3, 2)];\n",
    "     id2: [(w6, 1), (w7, 1), (w8, 2), (w9, 2), (w10, 2)];\n",
    "     ...}\n",
    "     \n",
    "     \n",
    "- task4: use the list of candidate replacements you found in task3 to find the best one according to candidate frequency (derived from SUBTLEXus) - if two or more candidates have the exact same frequency in SUBTLEX, choose the one with the best edit distance. If two or more candidates at the same frequency also have the same edit distance, pick the one which comes first in alphabetical order. You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its SUBTLEXus frequency value (as an integer). 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the frequency doesn't match, you loose half a point). Not all candidates might appear in the LM training corpus: don't map to the UNK string though, or the perplexity estimate would not reflect the specific candidate, you can directly exclude them from the perplexity computation.\n",
    "\n",
    "\n",
    "- task5: use the list of candidate replacements you found in task3 to find the best one according to its perplexity under a statistical language model of order 3 implemented using a Markov Chain with add-k smoothing (k=0.01) and estimated using the given corpus. If two or more candidates have the exact same perplexity in the input sentence, choose the one with the best edit distance. If two or more candidates at the same perplexity also have the same edit distance, follow alphabetical order. You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its perplexity under the specified language model (as a float). 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the perplexity doesn't match, you loose half a point).\n",
    "\n",
    "\n",
    "- task6: Compare the candidate replacements found when considering frequency and when considering perplexity. Which are better? Do they match what you consider to be the right replacement? What extra resources/information would you use to pick better candidate replacements? 3 points available, awarded based on how sensible the answer is.\n",
    "\n",
    "\n",
    "Name files as follows:\n",
    "task[n]_group[m]_solution.json\n",
    "\n",
    "such that the solutions from group 45 to task 2a should be contained in the file task2a_group45_solution.json, in the format specified in the task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a274bd",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "In the cell below you find code to run a statistical language model, add the docstrings (you already find a blueprint) to complete the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c603a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class LM(object):\n",
    "    \n",
    "    def __init__(self, corpus, ngram_size=2, bos='+', eos='#', k=1):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param corpus:      The collection of sentences to be used in the LM\n",
    "        :param ngram_size:  The size of word combinations to be created while preprocessing\n",
    "        :param k:           The fixed size to be added to avoid zero values (smoothing)\n",
    "        :param bos:         The symbol for the beginning of a sentence\n",
    "        :param eos:         The symbol for the end of a sentence\n",
    "        \n",
    "        This class creates an LM object with specific attributes \n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        self.ngram_size = ngram_size\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.corpus = corpus\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.vocab.add(self.eos)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: Returns the vocabulary, also in other words getting the different types.\n",
    "        \n",
    "        This method derives the vocabulary of our corpus\n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = set()\n",
    "        for sentence in self.corpus:\n",
    "            for element in sentence:\n",
    "                vocab.add(element)\n",
    "        \n",
    "        return vocab\n",
    "                    \n",
    "    def update_counts(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        :return: Does not return but updates the count global variable\n",
    "        \n",
    "        This method creates a dictionary for the counts of specific words given the context of the preciding n_gram parts.\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        \n",
    "        self.counts = defaultdict(dict)\n",
    "        \n",
    "        for sentence in self.corpus:\n",
    "            s = [self.bos]*r + sentence + [self.eos]\n",
    "            \n",
    "            for idx in range(self.ngram_size-1, len(s)):\n",
    "                ngram = self.get_ngram(s, idx)\n",
    "                \n",
    "                try:\n",
    "                    self.counts[ngram[0]][ngram[1]] += 1\n",
    "                except KeyError:\n",
    "                    self.counts[ngram[0]][ngram[1]] = 1\n",
    "                        \n",
    "    def get_ngram(self, s, i):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param s:   We get the sentence as input\n",
    "        :param i:   We get the index as input\n",
    "        :return:    Returns the the first parts of the n_gram and the last piece of the n_gram as target\n",
    "        \n",
    "        This method returns the n_gram having it's last word at the specificed index\n",
    "        \"\"\"\n",
    "        \n",
    "        ngram = s[i-(self.ngram_size-1):i+1]\n",
    "        history = tuple(ngram[:-1])\n",
    "        target = ngram[-1]\n",
    "        return (history, target)\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param history: The preceeding words of our n_gram\n",
    "        :param target:  The current/last word of our n_gram\n",
    "        :return:        Returns the probability of the target n_gram\n",
    "        \n",
    "        This method calculates our probability of the target word given the history of preceding words in our ngram\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values())) + (self.vocab_size*self.k)\n",
    "            try:\n",
    "                transition_count = self.counts[history][target] + self.k\n",
    "            except KeyError:\n",
    "                transition_count = self.k\n",
    "        except KeyError:\n",
    "            transition_count = self.k\n",
    "            ngram_tot = self.vocab_size*self.k\n",
    "        \n",
    "        return transition_count/ngram_tot \n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param string: the test sentence based on unseen data\n",
    "        :return: the perplexity (calculated value) on how our model performs\n",
    "        \n",
    "        This method computes the perplexity, thus a value showing the intrinsic evaluation of our model\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        s = [self.bos]*r + sentence + [self.eos]\n",
    "        \n",
    "\n",
    "        probs = []\n",
    "        for idx in range(self.ngram_size-1, len(s)):\n",
    "            ngram = self.get_ngram(s, idx)\n",
    "            probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "                    \n",
    "        entropy = np.log2(probs)\n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "075ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502f6a9",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8ea744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\friez\\AppData\\Local\\Temp\\ipykernel_10184\\415206871.py:3: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_sublexus = pd.read_csv(\"SUBTLEXus.txt\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df_with_typos = pd.read_csv(\"sentence_with_typos.txt\", sep=\"\\t\")\n",
    "df_sublexus = pd.read_csv(\"SUBTLEXus.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5bac9",
   "metadata": {},
   "source": [
    "## Task 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96402a60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'knoq', 2: 'reserachers', 3: 'grozn', 4: 'quolg', 5: 'waies', 6: 'wintr', 7: 'munors', 8: 'surgicaly', 9: 'aquire', 10: 'acomodate', 11: 'dats', 12: 'collegue', 13: 'layed', 14: 'cate', 15: 'giambic'}\n"
     ]
    }
   ],
   "source": [
    "# find mistyped words in the input sentences\n",
    "result_dict2 = {}\n",
    "\n",
    "#keepNan default\n",
    "for i in range(df_with_typos.shape[0]):\n",
    "    row = df_with_typos.loc[df_with_typos['ID'] == i+1, 'sentence'].item()\n",
    "    tokens = nltk.word_tokenize(row)\n",
    "    for token in tokens:\n",
    "        if token in \"?.,!\":\n",
    "            continue\n",
    "        if token != \"\" and token not in df_sublexus[\"Word\"].values:\n",
    "            result_dict2[i+1] = token\n",
    "\n",
    "print(result_dict2)\n",
    "    \n",
    "    \n",
    "with open(\"task2a_ChristopheFriezasGoncalves_solution.json\", \"w\") as outfile:\n",
    "    json.dump(result_dict2, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c013c0f",
   "metadata": {},
   "source": [
    "## Task2b\n",
    "\n",
    "In the first two sentences we have red and fiend which given the context of the sentences are also typo words yet they by themselves are correct words. One method would be a language model trained on a bigger corpus with sentences without typos to grasp the probabilities of words coming one after another. This model then may catch a word like red or fiend in this smaller corpus given that their probability of appearing inthe sentence after their specific sequence of world seems very very low thus marking them as typo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b126e",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c42f26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the nearest neighbors based on edit distance\n",
    "words = df_sublexus[\"Word\"].values\n",
    "dict_res = {}\n",
    "def set_dictEntry(idx, neighbors):\n",
    "    count=1\n",
    "    dict_res[idx]=[neighbors[0]]\n",
    "    for i in range(1,len(neighbors)-1):\n",
    "        if count >= 3 and neighbors[i-1][1]!=neighbors[i][1]:\n",
    "            break\n",
    "        count+=1\n",
    "        dict_res[idx].append(neighbors[i])\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "for i, typo in result_dict2.items():\n",
    "    target = typo\n",
    "    min_edit = 100  # just a very high number\n",
    "    neighbors = []\n",
    "    for word in words:\n",
    "        word = str(word)\n",
    "        if word != target:\n",
    "            d = nltk.edit_distance(word, target)\n",
    "            \n",
    "            if d>3:\n",
    "                continue\n",
    "            else:\n",
    "                neighbors.append((word,d))\n",
    "                neighbors.sort(key=lambda t: t[1])\n",
    "    \n",
    "    \n",
    "    set_dictEntry(i,neighbors)\n",
    "\n",
    "with open(\"task3_ChristopheFriezasGoncalves_solution.json\", \"w\") as outfile:\n",
    "    json.dump(dict_res, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9cda505-a307-4a1f-9271-af1b68f3b4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [('know', 1), ('knot', 1), ('knob', 1)], 2: [('researchers', 2), ('researcher', 3), ('researches', 3)], 3: [('grown', 1), ('groin', 1), ('groan', 1)], 4: [('quote', 2), ('quota', 2), ('quo', 2), ('quilt', 2), ('quell', 2), ('qualm', 2), ('quila', 2), ('quoad', 2), ('quoit', 2), ('quos', 2)], 5: [('waves', 1), ('wakes', 1), ('waits', 1), ('wages', 1), ('wails', 1), ('wares', 1), ('waxes', 1), ('wades', 1), ('waives', 1), ('waifs', 1), ('wanes', 1)], 6: [('winter', 1), ('wintry', 1), ('with', 2), ('want', 2), ('into', 2), ('went', 2), ('wants', 2), ('win', 2), ('wind', 2), ('wine', 2), ('winner', 2), ('wins', 2), ('wings', 2), ('wing', 2), ('minor', 2), ('hint', 2), ('diner', 2), ('winds', 2), ('ninth', 2), ('wit', 2), ('mint', 2), ('witty', 2), ('wits', 2), ('wiser', 2), ('pint', 2), ('wink', 2), ('finer', 2), ('wider', 2), ('windy', 2), ('intro', 2), ('hints', 2), ('wiener', 2), ('mints', 2), ('wilt', 2), ('wines', 2), ('miner', 2), ('liner', 2), ('pints', 2), ('lint', 2), ('winch', 2), ('wont', 2), ('width', 2), ('Pinto', 2), ('wino', 2), ('winks', 2), ('wiper', 2), ('minty', 2), ('winos', 2), ('inter', 2), ('tint', 2), ('dinar', 2), ('aint', 2), ('whiner', 2), ('wined', 2), ('winery', 2), ('wince', 2), ('Pinta', 2), ('entr', 2), ('winder', 2), ('dint', 2), ('bint', 2), ('int', 2), ('Sinter', 2), ('wanty', 2), ('wilts', 2), ('wite', 2), ('contr', 2), ('linty', 2), ('oint', 2), ('tints', 2), ('Winer', 2), ('winker', 2), ('Wint', 2), ('wintery', 2), ('wist', 2)], 7: [('minors', 1), ('minor', 2), ('rumors', 2), ('honors', 2), ('manor', 2), ('majors', 2), ('miners', 2), ('jurors', 2), ('donors', 2), ('tumors', 2), ('moors', 2), ('tutors', 2), ('juniors', 2), ('mucous', 2), ('monos', 2), ('mayors', 2), ('mentors', 2), ('tenors', 2), ('manos', 2), ('Manors', 2), ('humors', 2), ('lunars', 2), ('Senors', 2), ('Tuners', 2)], 8: [('surgical', 1), ('surgically', 1), ('strictly', 3), ('survival', 3)], 9: [('acquire', 1), ('Squire', 1), ('quire', 1)], 10: [('accomodate', 1), ('accommodate', 2), ('accommodated', 3), ('accommodates', 3)], 11: [('days', 1), ('date', 1), ('dates', 1), ('data', 1), ('cats', 1), ('eats', 1), ('rats', 1), ('hats', 1), ('bats', 1), ('dots', 1), ('dads', 1), ('oats', 1), ('darts', 1), ('mats', 1), ('fats', 1), ('pats', 1), ('dams', 1), ('dais', 1), ('tats', 1), ('Lats', 1), ('dabs', 1), ('gats', 1), ('vats', 1), ('doats', 1), ('Kats', 1), ('dato', 1), ('wats', 1)], 12: [('college', 1), ('colleague', 1), ('colleagues', 2), ('colleges', 2), ('collage', 2)], 13: [('played', 1), ('layer', 1), ('laced', 1), ('Fayed', 1), ('flayed', 1), ('slayed', 1), ('payed', 1), ('lamed', 1), ('lated', 1), ('lazed', 1)], 14: [('care', 1), ('came', 1), ('late', 1), ('case', 1), ('hate', 1), ('date', 1), ('cute', 1), ('cat', 1), ('ate', 1), ('cake', 1), ('rate', 1), ('gate', 1), ('fate', 1), ('mate', 1), ('cage', 1), ('cats', 1), ('cave', 1), ('cafe', 1), ('cane', 1), ('crate', 1), ('cater', 1), ('carte', 1), ('cite', 1), ('Tate', 1), ('pate', 1), ('caste', 1), ('sate', 1), ('bate', 1), ('Cate', 1), ('yate', 1)], 15: [('iambic', 1), ('gambit', 2), ('limbic', 2), ('iambics', 2)]}\n"
     ]
    }
   ],
   "source": [
    "print(dict_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f911707",
   "metadata": {},
   "source": [
    "## Task4: frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b809de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knoq ('know', 291780)\n",
      "reserachers ('researchers', 57)\n",
      "grozn ('grown', 1275)\n",
      "quolg ('quote', 488)\n",
      "waies ('waves', 674)\n",
      "wintr ('with', 257465)\n",
      "munors ('minor', 654)\n",
      "surgicaly ('strictly', 548)\n",
      "aquire ('Squire', 157)\n",
      "acomodate ('accommodate', 109)\n",
      "dats ('days', 15592)\n",
      "collegue ('college', 4344)\n",
      "layed ('played', 2870)\n",
      "cate ('care', 24748)\n",
      "giambic ('gambit', 19)\n"
     ]
    }
   ],
   "source": [
    "# pick the best candidate according to SUBTLEXus frequency counts\n",
    "freq = dict()\n",
    "words=df_sublexus[\"Word\"].values\n",
    "count=df_sublexus[\"FREQcount\"].values\n",
    "dict_res4 = {}\n",
    "for i in range(len(words)):\n",
    "    freq[words[i]] = count[i]\n",
    "    \n",
    "\n",
    "for i in range(1,16):\n",
    "    candid = dict_res[i]\n",
    "    best = []\n",
    "    \n",
    "    for j in candid:\n",
    "        word = j[0]\n",
    "        wordCom = (word,freq[word], j[1])\n",
    "        best.append(wordCom)\n",
    "    \n",
    "    best.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    \n",
    "    if best[0][1] == best[1][1]:\n",
    "        nw = [x for x in best if x[1] == best[0][1]] \n",
    "        nw.sort(key=lambda x: x[2])\n",
    "        \n",
    "        if nw[0][2] == nw[0][2]:\n",
    "            nw = [x for x in best if x[2] == nw[0][2] ]\n",
    "            nw.sort(key=lambda x: x[0])\n",
    "        best = nw\n",
    "    \n",
    "    dict_res4[i] = tuple((best[0][0],int(best[0][1])))\n",
    "    print(result_dict2[i],  dict_res4[i])\n",
    "\n",
    "\n",
    "with open(\"task4_ChristopheFriezasGoncalves_solution.json\", \"w\") as outfile:\n",
    "    json.dump(dict_res4, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024d5b2",
   "metadata": {},
   "source": [
    "## Task5: perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bc2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best candidate according to perplexity under the given language model\n",
    "import json\n",
    "\n",
    "with open(\"LM_trainingCorpus.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "647b85d0-b8d3-4cff-8c5f-ce667f314fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, n, corpus=None):\n",
    "                \n",
    "        self.sentences = corpus\n",
    "        self.ngram_size = n\n",
    "        \n",
    "        self.frequencies = self.freq_distr()\n",
    "        #self.filter_words()\n",
    "        self.add_bos_eos()\n",
    "        \n",
    "    def freq_distr(self):\n",
    "        count = Counter()\n",
    "        \n",
    "        for i in range(len(self.sentences)):\n",
    "            for word in self.sentences[i]:\n",
    "                count[word] += 1\n",
    "                \n",
    "        return count\n",
    "    \n",
    "    def filter_words(self):\n",
    "        for i in range(len(self.sentences)):\n",
    "            cleaned = []\n",
    "            for word in self.sentences[i]:\n",
    "                if word == \"\" or word in \".!?,\":\n",
    "                    continue\n",
    "                else:\n",
    "                    cleaned.append(word)\n",
    "                \n",
    "            self.sentences[i] = cleaned\n",
    "    \n",
    "    def add_bos_eos(self):\n",
    "        for i in range(len(self.sentences)):\n",
    "            new = ['#bos#']*(self.ngram_size-1)\n",
    "            new.extend(self.sentences[i])\n",
    "            new.append('#eos#')\n",
    "            self.sentences[i] = new\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "716ee26f-d836-4b59-806c-183918c7549a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(object):\n",
    "    \n",
    "    def __init__(self, n, smoother='Laplace', k=1, lambdas=None):\n",
    "        \n",
    "        # make sure that no fixed quantity is added to counts when the smoother is not Laplace\n",
    "        self.k = k\n",
    "        self.ngram_size = n\n",
    "        self.smoother = smoother\n",
    "        self.lambdas = lambdas if lambdas else {i+1: 1/n for i in range(n)}\n",
    "        \n",
    "    def get_ngram(self, sentence, i, n):\n",
    "        \n",
    "        if n == 1:\n",
    "            return sentence[i]\n",
    "        else:\n",
    "            ngram = sentence[i-(n-1):i+1]\n",
    "            history = tuple(ngram[:-1])\n",
    "            target = ngram[-1]\n",
    "            return (history, target)\n",
    "        \n",
    "                    \n",
    "    def update_counts(self, corpus, n):\n",
    "        \n",
    "        if self.ngram_size != corpus.ngram_size:\n",
    "            raise ValueError(\"The corpus was pre-processed considering an ngram size of {} while the \"\n",
    "                             \"language model was created with an ngram size of {}. \\n\"\n",
    "                             \"Please choose the same ngram size for pre-processing the corpus and fitting \"\n",
    "                             \"the model.\".format(corpus.ngram_size, self.ngram_size))\n",
    "        \n",
    "        self.counts = defaultdict(dict)\n",
    "        # if the interpolation smoother is selected, then estimate transition counts for all possible ngram_sizes\n",
    "        # smaller than the given one, otherwise stick with the input ngram_size\n",
    "        ngram_sizes = [n] if self.smoother != 'Interpolation' else range(1,n+1)\n",
    "        for ngram_size in ngram_sizes:\n",
    "            self.counts[ngram_size] = defaultdict(dict) if ngram_size > 1 else Counter()\n",
    "        for sentence in corpus.sentences:\n",
    "            for ngram_size in ngram_sizes:\n",
    "                for idx in range(n-1, len(sentence)):\n",
    "                    ngram = self.get_ngram(sentence, idx, ngram_size)\n",
    "                    if ngram_size == 1:\n",
    "                        self.counts[ngram_size][ngram] += 1\n",
    "                    else:\n",
    "                        # it's faster to try to do something and catch an exception than to use an if statement to \n",
    "                        # check whether a condition is met beforehand. The if is checked everytime, the exception \n",
    "                        # is only catched the first time, after that everything runs smoothly\n",
    "                        try:\n",
    "                            self.counts[ngram_size][ngram[0]][ngram[1]] += 1\n",
    "                        except KeyError:\n",
    "                            self.counts[ngram_size][ngram[0]][ngram[1]] = 1\n",
    "        \n",
    "        # first loop through the sentences in the corpus, than loop through each word in a sentence\n",
    "        self.vocab = {word for sentence in corpus.sentences for word in sentence}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "    \n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[self.ngram_size][history].values())) + (self.vocab_size*self.k)\n",
    "            try:\n",
    "                transition_count = self.counts[self.ngram_size][history][target] + self.k\n",
    "            except KeyError:\n",
    "                transition_count = self.k\n",
    "        except KeyError:\n",
    "            transition_count = self.k\n",
    "            ngram_tot = self.vocab_size*self.k\n",
    "            \n",
    "        return transition_count/ngram_tot \n",
    "    \n",
    "    def perplexity(self, test_corpus):\n",
    "        probs = []\n",
    "        for sentence in test_corpus.sentences:\n",
    "            for idx in range(self.ngram_size-1, len(sentence)):\n",
    "                ngram = self.get_ngram(sentence, idx, self.ngram_size)\n",
    "                probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "                        \n",
    "        entropy = np.log2(probs)\n",
    "\n",
    "        # this assertion makes sure that valid probabilities are retrieved, whose log must be <= 0\n",
    "        assert all(entropy <= 0)\n",
    "        \n",
    "        \n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        \n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7cb5961a-01c5-4943-b83d-215dfd97a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ('know', 6121.518964425194), 2: ('researchers', 44977.18970730794), 3: ('groan', 7770.120723574704), 4: ('qualm', 74762.30658062035), 5: ('wades', 6420.0778660793085), 6: ('winter', 14222.749639803715), 7: ('jurors', 44017.860939303704), 8: ('surgical', 66731.89255192831), 9: ('Squire', 33355.42808237516), 10: ('accomodate', 49093.49848525223), 11: ('Kats', 40273.413009451724), 12: ('colleague', 2727.071187173628), 13: ('Fayed', 39395.67041280543), 14: ('care', 21888.81399625804), 15: ('iambic', 71457.10634471412)}\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(4,data)\n",
    "ngram_model = LM(4, k=0.01, lambdas=None)\n",
    "ngram_model.update_counts(corpus, 4)\n",
    "dict_res5 = {}\n",
    "sentences = {}\n",
    "\n",
    "\n",
    "for i in range(df_with_typos.shape[0]):\n",
    "    row = df_with_typos.loc[df_with_typos['ID'] == i+1, 'sentence'].item()\n",
    "    tokens = nltk.word_tokenize(row)\n",
    "    sentences[i+1]=tokens\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1,16):\n",
    "    candid = dict_res[i]\n",
    "    best = []\n",
    "    replace = result_dict2[i]\n",
    "    sentence = sentences[i]\n",
    "    index = sentence.index(replace)\n",
    "    \n",
    "    \n",
    "    for j in candid:\n",
    "        repSentence = sentence[:]\n",
    "        word = j[0]\n",
    "        repSentence[index] = word\n",
    "        wordCom = (word, ngram_model.perplexity(Corpus(4,[repSentence])), j[1])\n",
    "        best.append(wordCom)\n",
    "    \n",
    "    best.sort(key=lambda x: x[1])\n",
    "    \n",
    "    \n",
    "    if best[0][1] == best[1][1]:\n",
    "        nw = [x for x in best if x[1] == best[0][1]] \n",
    "        nw.sort(key=lambda x: x[2])\n",
    "        \n",
    "        if nw[0][2] == nw[0][2]:\n",
    "            nw = [x for x in best if x[2] == nw[0][2] ]\n",
    "            nw.sort(key=lambda x: x[0])\n",
    "        best = nw\n",
    "    \n",
    "    dict_res5[i] = (best[0][0],best[0][1])\n",
    "    \n",
    "\n",
    "print(dict_res5)\n",
    "with open(\"task5_ChristopheFriezasGoncalves_solution.json\", \"w\") as outfile:\n",
    "    json.dump(dict_res5, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513369b",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "As we see for the 15 sentences frequency based typo replacement gives 5 out of 15 correct answers and perplexity gives 8 out of 15 correct answers. The perplexity thus shows better results when considering tokens based on edit distances. Adding that frequency does not take any context into account, just the fact that it may be a word that occurs often. Now we do not have any background on the training corpus thus we may improve our Language model if we may use a corpus that is based more on our sentences and their context. Given that we are looking for specific words for typos, lemmatizing and normalizing other than taking capitalization away is not possible, yet we can still try and fine tune our model, try different ngram sizes or train the model itself with more of unknown words tags, thus words like Squire, Fayed as seen in task 5 won't show up as option, given that squire and Fayed are not very common words in everyday language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc92eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
